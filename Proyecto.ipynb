{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Proyecto.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI3mWAK3XYqa",
        "outputId": "046f57e3-8dc1-413e-d1e8-76fad5012a7d"
      },
      "source": [
        "pip install qgrid"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: qgrid in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: notebook>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from qgrid) (5.3.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from qgrid) (1.1.5)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from qgrid) (7.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.0.0->qgrid) (5.6.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook>=4.0.0->qgrid) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.0.0->qgrid) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.0.0->qgrid) (4.7.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook>=4.0.0->qgrid) (5.0.8)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.0.0->qgrid) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.0.0->qgrid) (5.3.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook>=4.0.0->qgrid) (4.10.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.0.0->qgrid) (1.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.0.0->qgrid) (2.11.2)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.0.0->qgrid) (0.9.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->qgrid) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->qgrid) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->qgrid) (2.8.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->qgrid) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->qgrid) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->qgrid) (3.5.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (3.2.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (1.4.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (0.8.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (2.6.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (0.4.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.0.0->qgrid) (0.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook>=4.0.0->qgrid) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook>=4.0.0->qgrid) (1.15.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook>=4.0.0->qgrid) (2.6.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.0->notebook>=4.0.0->qgrid) (20.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.0.0->qgrid) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook>=4.0.0->qgrid) (0.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->qgrid) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->qgrid) (51.1.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->qgrid) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->qgrid) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->qgrid) (1.0.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.0.0->qgrid) (20.8)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.0.0->qgrid) (0.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->qgrid) (0.2.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert->notebook>=4.0.0->qgrid) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N22jkcs7RlZ-"
      },
      "source": [
        "#imports\n",
        "import math\n",
        "import scipy as sc\n",
        "import numpy.matlib\n",
        "import numpy as np\n",
        "import operator\n",
        "import pandas as pd\n",
        "import qgrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from numpy import random\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "from sklearn.feature_selection import RFE\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDYLrErDRlaI",
        "outputId": "dca186db-da7f-4167-8c84-910f23ba425c"
      },
      "source": [
        "from sklearn import datasets\n",
        "db = np.loadtxt('./letter-recognition.data',delimiter=',',skiprows=0,dtype=bytes).astype(str)\n",
        "X = db[0::, 1::] # extracting\n",
        "Y = db[0::,0]\n",
        "j=0;\n",
        "for i in Y:\n",
        "    Y[j]=ord(i) #chr(i)\n",
        "    j+=1\n",
        "data=X.astype(float)\n",
        "Y=Y.astype(float)\n",
        "X=X.astype(float)\n",
        "print(Y)\n",
        "print(X)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[84. 73. 68. ... 84. 83. 65.]\n",
            "[[ 2.  8.  3. ...  8.  0.  8.]\n",
            " [ 5. 12.  3. ...  8.  4. 10.]\n",
            " [ 4. 11.  6. ...  7.  3.  9.]\n",
            " ...\n",
            " [ 6.  9.  6. ... 12.  2.  4.]\n",
            " [ 2.  3.  4. ...  9.  5.  8.]\n",
            " [ 4.  9.  6. ...  7.  2.  8.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z668kJoCRlaJ",
        "outputId": "a3e0868f-aad1-481e-b20e-6e825c06c747"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#Naive Bayes\n",
        "#Validamos el modelo\n",
        "Folds = 4\n",
        "random.seed(19680801)\n",
        "ETrain = np.zeros(Folds)\n",
        "EVal = np.zeros(Folds)\n",
        "Eficiencia=np.zeros(Folds)\n",
        "Precision=np.zeros(Folds)\n",
        "Sensibilidad=np.zeros(Folds)\n",
        "Fscore=np.zeros(Folds)\n",
        "skf = StratifiedKFold(n_splits=Folds)\n",
        "j = 0\n",
        "model = MultinomialNB()\n",
        "\n",
        "for train, test in skf.split(X, Y):\n",
        "    Xtrain = X[train,:]\n",
        "    Ytrain = Y[train]\n",
        "    Xtest = X[test,:]\n",
        "    Ytest = Y[test]\n",
        "    print(Ytest)\n",
        "    #Normalizamos los datos\n",
        "    media = np.mean(Xtrain)\n",
        "    desvia = np.std(Xtrain)\n",
        "    #Haga el llamado a la función para crear y entrenar el modelo usando los datos de entrenamiento\n",
        "    model.fit(Xtrain,Ytrain)  \n",
        "    #Validación\n",
        "    Ytrain_pred = model.predict(Xtrain)\n",
        "    Yest = model.predict(Xtest)\n",
        "    print(Yest)\n",
        "    print(Yest.shape)\n",
        "    #Evaluamos las predicciones del modelo con los datos de test\n",
        "    \n",
        "    Eficiencia[j] = accuracy_score(Ytest, Yest)\n",
        "    Precision[j] =np.mean(precision_score(Ytest, Yest, average=None))\n",
        "    Sensibilidad[j]=np.mean(recall_score(Ytest, Yest, average=None))\n",
        "    Fscore[j]=np.mean(f1_score(Ytest, Yest, average=None))\n",
        "    \n",
        "    ETrain[j] = np.mean(Ytrain_pred.ravel() == Ytrain.ravel())\n",
        "    EVal[j] = np.mean(Yest.ravel() == Ytest.ravel())    \n",
        "    j += 1\n",
        "print('Eficiencia= '+str(Eficiencia)) \n",
        "print('Precisiòn= '+str(Precision)) \n",
        "print('Sensibilidad='+str(Sensibilidad)) \n",
        "print('Fscore= '+str(Fscore)) \n",
        "print('Eficiencia durante el entrenamiento = ' + str(np.mean(ETrain)) + '+-' + str(np.std(ETrain)))\n",
        "print('Eficiencia durante la validación = ' + str(np.mean(EVal)) + '+-' + str(np.std(EVal)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[84. 73. 68. ... 87. 87. 87.]\n",
            "[73. 65. 65. ... 87. 87. 87.]\n",
            "(5000,)\n",
            "[74. 74. 72. ... 87. 87. 87.]\n",
            "[74. 74. 88. ... 77. 87. 87.]\n",
            "(5000,)\n",
            "[72. 84. 72. ... 81. 82. 81.]\n",
            "[75. 66. 72. ... 81. 67. 81.]\n",
            "(5000,)\n",
            "[66. 66. 86. ... 84. 83. 65.]\n",
            "[66. 66. 80. ... 84. 90. 65.]\n",
            "(5000,)\n",
            "Eficiencia= [0.5566 0.554  0.5578 0.545 ]\n",
            "Precisiòn= [0.56493613 0.55649172 0.56124294 0.54879155]\n",
            "Sensibilidad=[0.55522429 0.55251394 0.55593474 0.54365455]\n",
            "Fscore= [0.54482405 0.54191875 0.54597781 0.53180846]\n",
            "Eficiencia durante el entrenamiento = 0.5536833333333333+-0.0020866640042581474\n",
            "Eficiencia durante la validación = 0.55335+-0.005012733785071748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK4LzEwoRlaM"
      },
      "source": [
        "#Gradient boosting tree\n",
        "def entrenamientoRF(arboles , variablesPorNodo, modelo):\n",
        "    from sklearn.ensemble import RandomForestClassifier as RF\n",
        "    from sklearn.ensemble import BaggingClassifier as BC\n",
        "\n",
        "    #Validamos el modelo\n",
        "    Folds = 4\n",
        "    random.seed(19680801)\n",
        "    ETrain = np.zeros(Folds)\n",
        "    EVal = np.zeros(Folds)\n",
        "    Eficiencia=np.zeros(Folds)\n",
        "    Precision=np.zeros(Folds)\n",
        "    Sensibilidad=np.zeros(Folds)\n",
        "    Fscore=np.zeros(Folds)\n",
        "    skf = StratifiedKFold(n_splits=Folds)\n",
        "    j = 0\n",
        "    for train, test in skf.split(X, Y):\n",
        "        Xtrain = X[train,:]\n",
        "        Ytrain = Y[train]\n",
        "        Xtest = X[test,:]\n",
        "        Ytest = Y[test]\n",
        "\n",
        "        #Normalizamos los datos\n",
        "        media = np.mean(Xtrain)\n",
        "        desvia = np.std(Xtrain)\n",
        "        #Haga el llamado a la función para crear y entrenar el modelo usando los datos de entrenamiento\n",
        "        if modelo==0:\n",
        "            #Haga el llamado a la función para crear y entrenar el modelo usando los datos de entrenamiento\n",
        "            model = RF(n_estimators=arboles,max_features=variablesPorNodo)  \n",
        "            model.fit(Xtrain,Ytrain)\n",
        "            #Validación\n",
        "            Ytrain_pred = model.predict(Xtrain)\n",
        "            Yest = model.predict(Xtest) \n",
        "        if modelo==1:\n",
        "            model = BC(n_estimators=arboles,max_features=variablesPorNodo*16 )\n",
        "            model.fit(Xtrain,Ytrain)\n",
        "            #Validación\n",
        "            Ytrain_pred = model.predict(Xtrain)\n",
        "            Yest = model.predict(Xtest) \n",
        "        \n",
        "        Eficiencia[j] = accuracy_score(Ytest, Yest)\n",
        "        Precision[j] =np.mean(precision_score(Ytest, Yest, average=None))\n",
        "        Sensibilidad[j]=np.mean(recall_score(Ytest, Yest, average=None))\n",
        "        Fscore[j]=np.mean(f1_score(Ytest, Yest, average=None))\n",
        "    \n",
        "        #Evaluamos las predicciones del modelo con los datos de test\n",
        "        ETrain[j] = np.mean(Ytrain_pred.ravel() == Ytrain.ravel())\n",
        "        EVal[j] = np.mean(Yest.ravel() == Ytest.ravel())\n",
        "        j += 1\n",
        "\n",
        "    \n",
        "    eficiencia = np.mean(EVal)\n",
        "    intervaloConfianza = np.std(EVal)\n",
        "    return eficiencia, intervaloConfianza, np.mean(Eficiencia),np.mean(Precision), np.mean(Sensibilidad), np.mean(Fscore)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjDugM3MRlaP"
      },
      "source": [
        "#Calculando Numero de arboles por Variables analizadas por nodo\n",
        "randn = np.random.randn\n",
        "df_types2 = pd.DataFrame({\n",
        "    'Numero de arboles' : pd.Series([5,5,5,5,5,5,10,10,10,10,10,10,20,20,20,20,20,20,50,50,50,50,50,50,100,100,100,100,100,100]), 'Variables analizadas por nodo' : pd.Series([3,6,9,12,15,16,3,6,9,12,15,16,3,6,9,12,15,16,3,6,9,12,15,16,3,6,9,12,15,16])})\n",
        "df_types2[\"Eficiencia en validacion\"] = \"\"\n",
        "df_types2[\"Intervalo de confianza\"] = \"\"\n",
        "df_types2[\"Eficiencia\"] = \"\"\n",
        "df_types2[\"Precisiòn\"] = \"\"\n",
        "df_types2[\"Sensibilidad\"] = \"\"\n",
        "df_types2[\"Fscore\"] = \"\"\n",
        "#df_types.sort_index(inplace=True)\n",
        "df_types2.set_index(['Numero de arboles','Variables analizadas por nodo'], inplace=True)\n",
        "\n",
        "arrEficienciaVal = []\n",
        "arrInConfianza= []\n",
        "arrEficiencia=[]\n",
        "arrPrecision=[]\n",
        "arrSensibilidad=[]\n",
        "arrFscore=[]\n",
        "for item in df_types2.index:\n",
        "    eficienciaVal, intervaloConfianza,Eficiencia, Precision, Sensibilidad, Fscore= entrenamientoRF(int(item[0]),int(item[1]),0)\n",
        "    arrEficienciaVal.append(eficienciaVal)\n",
        "    arrInConfianza.append(intervaloConfianza)\n",
        "    arrEficiencia.append(Eficiencia)\n",
        "    arrPrecision.append(Precision)\n",
        "    arrSensibilidad.append(Sensibilidad)\n",
        "    arrFscore.append(Fscore)\n",
        "    \n",
        "df_types2[\"Eficiencia en validacion\"] = arrEficienciaVal\n",
        "df_types2[\"Intervalo de confianza\"] = arrInConfianza\n",
        "df_types2[\"Eficiencia\"] = arrEficiencia\n",
        "df_types2[\"Precisiòn\"] = arrPrecision\n",
        "df_types2[\"Sensibilidad\"] = arrSensibilidad\n",
        "df_types2[\"Fscore\"] = arrFscore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPxyDHhFRlaQ"
      },
      "source": [
        "qgrid_widget = qgrid.show_grid(df_types2, show_toolbar=False)\n",
        "qgrid_widget\n",
        "qgrid_widget.get_changed_df()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1QnEUFqRlaU"
      },
      "source": [
        "#Redes Neuronales Artificiales\n",
        "def entrenamientoRNA(nroCapasOcultas, neuronasPorCapa):\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "    Folds = 4\n",
        "    random.seed(19680801)\n",
        "    ETrain = np.zeros(Folds)\n",
        "    EVal = np.zeros(Folds)\n",
        "    Eficiencia=np.zeros(Folds)\n",
        "    Precision=np.zeros(Folds)\n",
        "    Sensibilidad=np.zeros(Folds)\n",
        "    Fscore=np.zeros(Folds)\n",
        "    skf = StratifiedKFold(n_splits=Folds)\n",
        "    j = 0\n",
        "    for train, test in skf.split(X, Y):\n",
        "        Xtrain = X[train,:]\n",
        "        Ytrain = Y[train]\n",
        "        Xtest = X[test,:]\n",
        "        Ytest = Y[test]\n",
        "\n",
        "        #Normalizamos los datos\n",
        "        media = np.mean(Xtrain)\n",
        "        desvia = np.std(Xtrain)\n",
        "        #Haga el llamado a la función para crear y entrenar el modelo usando los datos de entrenamiento\n",
        "\n",
        "        if(nroCapasOcultas==1):\n",
        "            mlp=MLPClassifier(activation='tanh',max_iter = 700,hidden_layer_sizes=(neuronasPorCapa))\n",
        "        else:\n",
        "            mlp=MLPClassifier(activation='tanh',max_iter = 700,hidden_layer_sizes=(neuronasPorCapa,neuronasPorCapa))\n",
        "        \n",
        "        mlp.fit(Xtrain,Ytrain)\n",
        "        \n",
        "        Ytrain_pred = mlp.predict(Xtrain)\n",
        "        Yest = mlp.predict(Xtest)          \n",
        "        \n",
        "        Eficiencia[j] = accuracy_score(Ytest, Yest)\n",
        "        Precision[j] =np.mean(precision_score(Ytest, Yest, average=None))\n",
        "        Sensibilidad[j]=np.mean(recall_score(Ytest, Yest, average=None))\n",
        "        Fscore[j]=np.mean(f1_score(Ytest, Yest, average=None))\n",
        "    \n",
        "        #Evaluamos las predicciones del modelo con los datos de test\n",
        "        EVal[j] = np.mean(Yest.ravel() == Ytest.ravel())\n",
        "        j += 1\n",
        "\n",
        "    \n",
        "    eficiencia = np.mean(EVal)\n",
        "    intervaloConfianza = np.std(EVal)\n",
        "    return eficiencia, intervaloConfianza, np.mean(Eficiencia),np.mean(Precision), np.mean(Sensibilidad), np.mean(Fscore)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHi2gGYgRlaV"
      },
      "source": [
        "#Aplicando Redes Neuronales Artificiales \n",
        "df_types2 = pd.DataFrame({\n",
        "    'N. de capas ocultas' : pd.Series([1,1,1,1,1,2,2,2,2,2]),\n",
        "    'Neuronas por capa' : pd.Series([20,24,28,32,36,20,24,28,32,36])})\n",
        "df_types2[\"Eficiencia en validacion\"] = \"\"\n",
        "df_types2[\"Intervalo de confianza\"] = \"\"\n",
        "df_types2[\"Eficiencia\"] = \"\"\n",
        "df_types2[\"Precisiòn\"] = \"\"\n",
        "df_types2[\"Sensibilidad\"] = \"\"\n",
        "df_types2[\"Fscore\"] = \"\"\n",
        "\n",
        "df_types2.set_index(['N. de capas ocultas','Neuronas por capa'], inplace=True)\n",
        "\n",
        "arrEficienciaVal = []\n",
        "arrInConfianza= []\n",
        "arrEficiencia=[]\n",
        "arrPrecision=[]\n",
        "arrSensibilidad=[]\n",
        "arrFscore=[]\n",
        "\n",
        "for h in df_types2.index:\n",
        "    eficienciaVal, intervaloConfianza,Eficiencia, Precision, Sensibilidad, Fscore = entrenamientoRNA(int(h[0]),int(h[1]))   \n",
        "    arrEficienciaVal.append(eficienciaVal)\n",
        "    arrInConfianza.append(intervaloConfianza)\n",
        "    arrEficiencia.append(Eficiencia)\n",
        "    arrPrecision.append(Precision)\n",
        "    arrSensibilidad.append(Sensibilidad)\n",
        "    arrFscore.append(Fscore)\n",
        "        \n",
        "df_types2[\"Eficiencia en validacion\"] = arrEficienciaVal\n",
        "df_types2[\"Intervalo de confianza\"] = arrInConfianza\n",
        "df_types2[\"Eficiencia\"] = arrEficiencia\n",
        "df_types2[\"Precisiòn\"] = arrPrecision\n",
        "df_types2[\"Sensibilidad\"] = arrSensibilidad\n",
        "df_types2[\"Fscore\"] = arrFscore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCm_ctDbRlaY"
      },
      "source": [
        "qgrid_widget = qgrid.show_grid(df_types2, show_toolbar=False)\n",
        "qgrid_widget\n",
        "qgrid_widget.get_changed_df()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb3RlDZhRlaY"
      },
      "source": [
        "#Maquina de vectores\n",
        "from sklearn import preprocessing\n",
        "from sklearn.svm import SVC\n",
        "def entrenamientoMV(kernel, c, gamma):\n",
        "\n",
        "    Folds = 4\n",
        "    random.seed(19680801)\n",
        "    ETrain = np.zeros(Folds)\n",
        "    EVal = np.zeros(Folds)\n",
        "    Eficiencia=np.zeros(Folds)\n",
        "    Precision=np.zeros(Folds)\n",
        "    Sensibilidad=np.zeros(Folds)\n",
        "    Fscore=np.zeros(Folds)\n",
        "    Porcentaje=np.zeros(Folds)\n",
        "    skf = StratifiedKFold(n_splits=Folds)\n",
        "    j = 0\n",
        "    for train, test in skf.split(X, Y):   \n",
        "        Xtrain = X[train,:]\n",
        "        Ytrain = Y[train]\n",
        "        Xtest = X[test,:]\n",
        "        Ytest = Y[test]\n",
        "\n",
        "        #Normalizamos los datos\n",
        "        media = np.mean(Xtrain)\n",
        "        desvia = np.std(Xtrain)\n",
        "\n",
        "        #Haga el llamado a la función para crear y entrenar el modelo usando los datos de entrenamiento        \n",
        "        if( kernel == 'linear'):\n",
        "            modelo = SVC(kernel= kernel, C=c, decision_function_shape='ovo')\n",
        "        else:\n",
        "            modelo = SVC(kernel= kernel, C=c, gamma=gamma, decision_function_shape='ovo') \n",
        "      \n",
        "        modelo.fit(Xtrain,Ytrain)\n",
        "        \n",
        "        Ytrain_pred = modelo.predict(Xtrain)       \n",
        "        Yest = modelo.predict(Xtest)          \n",
        "        \n",
        "        Eficiencia[j] = accuracy_score(Ytest, Yest)\n",
        "        Precision[j] =np.mean(precision_score(Ytest, Yest, average=None))\n",
        "        Sensibilidad[j]=np.mean(recall_score(Ytest, Yest, average=None))\n",
        "        Fscore[j]=np.mean(f1_score(Ytest, Yest, average=None))\n",
        "    \n",
        "        #Evaluamos las predicciones del modelo con los datos de test\n",
        "        EVal[j] = np.mean(Yest.ravel() == Ytest.ravel())\n",
        "        Porcentaje[j] =  len(modelo.support_vectors_) / len(Xtrain)\n",
        "        j += 1\n",
        "\n",
        "    \n",
        "    eficiencia = np.mean(EVal)\n",
        "    intervaloConfianza = np.std(EVal)\n",
        "    porcentaje = np.mean(Porcentaje)\n",
        "    \n",
        "    return eficiencia, intervaloConfianza, np.mean(Eficiencia),np.mean(Precision), np.mean(Sensibilidad), np.mean(Fscore), porcentaje"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnyLdPuvRlaZ"
      },
      "source": [
        "#Aplicando maquinas de Vectores\n",
        "df_types2 = pd.DataFrame({\n",
        "    'Kernel' : pd.Series(['linear','linear','linear','linear','linear','linear','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf','rbf']),\n",
        "    'C' : pd.Series([0.001,0.01,0.1,1,10,100,0.001,0.001,0.001,0.01,0.01,0.01,0.1,0.1,0.1,1,1,1,10,10,10,100,100,100]),\n",
        "    'gamma' : pd.Series([0,0,0,0,0,0,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1,0.01,0.1,1])})\n",
        "df_types2[\"% de Vectores de Soporte\"] = \"\"\n",
        "df_types2[\"Eficiencia en validacion\"] = \"\"\n",
        "df_types2[\"Intervalo de confianza\"] = \"\"\n",
        "df_types2[\"Eficiencia\"] = \"\"\n",
        "df_types2[\"Precisiòn\"] = \"\"\n",
        "df_types2[\"Sensibilidad\"] = \"\"\n",
        "df_types2[\"Fscore\"] = \"\"\n",
        "\n",
        "df_types2.set_index(['Kernel','C','gamma'], inplace=True)\n",
        "#df_types.sort_index(inplace=True)\n",
        "\n",
        "arrEficienciaVal = []\n",
        "arrInConfianza= []\n",
        "arrEficiencia=[]\n",
        "arrPrecision=[]\n",
        "arrSensibilidad=[]\n",
        "arrFscore=[]\n",
        "arrPorcentaje=[]\n",
        "\n",
        "for h in df_types2.index:\n",
        "    eficienciaVal, intervaloConfianza,Eficiencia, Precision, Sensibilidad, Fscore, porcentaje = entrenamientoMV(h[0],h[1],h[2])   \n",
        "    arrEficienciaVal.append(eficienciaVal)\n",
        "    arrInConfianza.append(intervaloConfianza)\n",
        "    arrEficiencia.append(Eficiencia)\n",
        "    arrPrecision.append(Precision)\n",
        "    arrSensibilidad.append(Sensibilidad)\n",
        "    arrFscore.append(Fscore)\n",
        "    arrPorcentaje.append(porcentaje)\n",
        "        \n",
        "df_types2[\"Eficiencia en validacion\"] = arrEficienciaVal\n",
        "df_types2[\"Intervalo de confianza\"] = arrInConfianza\n",
        "df_types2[\"Eficiencia\"] = arrEficiencia\n",
        "df_types2[\"Precisiòn\"] = arrPrecision\n",
        "df_types2[\"Sensibilidad\"] = arrSensibilidad\n",
        "df_types2[\"Fscore\"] = arrFscore\n",
        "df_types2[\"% de Vectores de Soporte\"] = arrPorcentaje"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrQfoANuRlaa"
      },
      "source": [
        "qgrid_widget = qgrid.show_grid(df_types2, show_toolbar=False)\n",
        "qgrid_widget\n",
        "qgrid_widget.get_changed_df()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INlXDHedRlab"
      },
      "source": [
        "def entrenamiento_sin_seleccion_caracteristicas(splits, X, Y):\n",
        "   \n",
        "    #Implemetamos la metodología de validación\n",
        "    Errores = np.ones(splits)\n",
        "    Score = np.ones(splits)\n",
        "    times = np.ones(splits)\n",
        "    j = 0\n",
        "    kf = KFold(n_splits=splits)\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = Y[train_index], Y[test_index]\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "        #Creamos el clasificador SVM.\n",
        "        clf = SVC(kernel=\"linear\", C=1)\n",
        "        #Aquí se entran y se valida el modelo sin hacer selección de características\n",
        "        ######\n",
        "        # Entrenamiento el modelo.\n",
        "        #Para calcular el costo computacional\n",
        "        tiempo_i = time.time()\n",
        "        clf.fit(X_train,y_train)\n",
        "        # Validación del modelo\n",
        "        Errores[j] = accuracy_score(y_true=y_test, y_pred=clf.predict(X_test))\n",
        "        times[j] = time.time()-tiempo_i\n",
        "        j+=1\n",
        "\n",
        "    return clf, np.mean(Errores), np.std(Errores), np.mean(times)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0-LDF2mRlab"
      },
      "source": [
        "def recursive_feature_elimination_wrapper(estimator, feature_numbers, X,Y):\n",
        "    rfe = RFE(estimator=estimator, step=1,n_features_to_select=feature_numbers)\n",
        "    tiempo_i = time.time()\n",
        "    rfe.fit(X=X,y=Y)\n",
        "    time_o = time.time()-tiempo_i\n",
        "    feature_mask = rfe.support_\n",
        "    #print(feature_mask)\n",
        "    features_rank = rfe.ranking_\n",
        "    #print(features_rank)\n",
        "    estimator = rfe.estimator_\n",
        "    #print(estimator)\n",
        "    #print(rfe)\n",
        "    \n",
        "    return rfe, feature_mask, features_rank, estimator, time_o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh2OmqW-Rlac"
      },
      "source": [
        "def experimentar(n_feats, n_sets, X, Y):\n",
        "    df = pd.DataFrame()\n",
        "    idx = 0\n",
        "    for split_number in n_sets: \n",
        "    #Sin selección de características\n",
        "        # se ignorar las otras salidas\n",
        "        _,err,ic,t_ex = entrenamiento_sin_seleccion_caracteristicas(split_number, X,Y)  \n",
        "        df.loc[idx,'CON_SEL'] = 'NO'\n",
        "        df.loc[idx,'NUM_VAR'] = X.shape[1]\n",
        "        df.loc[idx,'NUM_SPLITS'] = split_number\n",
        "        df.loc[idx,'ERROR_VALIDACION'] = err\n",
        "        df.loc[idx,'IC_STD_VALIDACION'] = ic\n",
        "        df.loc[idx,'T_EJECUCION'] = t_ex\n",
        "        idx+=1\n",
        "    print(\"termina experimentos sin selección\")\n",
        "    #Con selección de características\n",
        "    for f in n_feats:\n",
        "        for split_number in n_sets:\n",
        "            #Implemetamos la metodología de validación \n",
        "            Errores = np.ones(split_number)\n",
        "            Score = np.ones(split_number)\n",
        "            times = np.ones(split_number)\n",
        "            kf = KFold(n_splits=split_number)\n",
        "            j = 0\n",
        "            for train_index, test_index in kf.split(X):\n",
        "                \n",
        "                X_train, X_test = X[train_index], X[test_index]\n",
        "                y_train, y_test = Y[train_index], Y[test_index]\n",
        "                scaler = StandardScaler()\n",
        "                X_train = scaler.fit_transform(X_train)\n",
        "                X_test = scaler.transform(X_test)\n",
        "                \n",
        "                svc =  SVC(kernel=\"linear\", C=1)\n",
        "                \n",
        "                # se ignorar las otras salidas\n",
        "                rfe, _, _, _, t = recursive_feature_elimination_wrapper(estimator=svc,feature_numbers=f, X=X_test,Y=y_test)\n",
        "            \n",
        "                Errores[j]=accuracy_score(y_true = y_test,  y_pred=rfe.predict(X_test))\n",
        "                times[j] = t\n",
        "                j+=1\n",
        "\n",
        "            df.loc[idx,'CON_SEL'] = 'SI'\n",
        "            df.loc[idx,'NUM_VAR'] = f\n",
        "            df.loc[idx,'NUM_SPLITS'] = split_number\n",
        "            df.loc[idx,'ERROR_VALIDACION'] = np.mean(Errores)\n",
        "            df.loc[idx, 'IC_STD_VALIDACION'] = np.std(Errores)\n",
        "            df.loc[idx, 'T_EJECUCION'] = np.mean(times)\n",
        "            idx+=1\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rb0EWJ-WRlac"
      },
      "source": [
        "dfr = experimentar(n_feats = [3, 5,7, 10,15,17], n_sets = [5, 10], X= X, Y=Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJrkT95ARlad"
      },
      "source": [
        "dfr.sort_values(['ERROR_VALIDACION','T_EJECUCION'], ascending=[False, True])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eZPKe0ORlad"
      },
      "source": [
        "svc =  SVC(kernel=\"linear\", C=1)\n",
        "rfe, feature_mask, _, _, _ = recursive_feature_elimination_wrapper(svc,17, X,Y)\n",
        "print(\"esta es la mascara (deberia ser solo valores True y False) \\n\", feature_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2aYu4o9Rlad"
      },
      "source": [
        "#Indice de fisher\r\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\r\n",
        "valores = f_classif( X , Y )\r\n",
        "print(valores)\r\n",
        "\r\n",
        "X_new = SelectKBest(f_classif, k=1).fit(X, Y)\r\n",
        "caracteristicas=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\r\n",
        "atrib = X_new.get_support()\r\n",
        "params=X_new.get_params()\r\n",
        "atributos = [caracteristicas[i] for i in list(atrib.nonzero()[0])]\r\n",
        "print(atrib)\r\n",
        "atributos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mTY0jWxo61Y"
      },
      "source": [
        "def MAPE(Y_est,Y):\r\n",
        "    \"\"\"Mean Absolute Percentage Error para los problemas de regresión\r\n",
        "    Y_est: numpy array con los valores estimados\r\n",
        "    Y: numpy array con las etiquetas verdaderas\r\n",
        "    retorna: mape\r\n",
        "    \"\"\"\r\n",
        "    N = np.size(Y)\r\n",
        "    mape = np.sum(abs((Y_est.reshape(N,1) - Y.reshape(N,1))/Y.reshape(N,1)))/N\r\n",
        "    return mape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE68a0PRo8Ke"
      },
      "source": [
        "def kernel_gaussiano(x):\r\n",
        "    return (np.exp((-0.5)*x**2))\r\n",
        "\r\n",
        "def ParzenWindow(x,Data,h,Datay=None):\r\n",
        "    \"\"\"\"ventana de parzen\r\n",
        "    x: vector con representando una sola muestra\r\n",
        "    Data: vector de muestras de entrenamiento\r\n",
        "    h: ancho de la ventana de kernel\r\n",
        "    Datay: vector con los valores de salida (y), Si no se pasa como argumento, \r\n",
        "        se calcula un ventana de parzen sin multiplicar los valores de este vector.\r\n",
        "    retorna: el valor de ventana de parzen para una muestra\r\n",
        "    \"\"\"\r\n",
        "    h = h\r\n",
        "    Ns = Data.shape[0]\r\n",
        "    suma = 0\r\n",
        "    for k in range(Ns):\r\n",
        "        u = euclidean(x,Data[k,:])\r\n",
        "        if Datay is None:\r\n",
        "            suma += kernel_gaussiano(u/h)\r\n",
        "        else:\r\n",
        "            suma += kernel_gaussiano(u/h)*Datay[k]\r\n",
        "    return suma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOkWlZIao_ZG"
      },
      "source": [
        "def Nadaraya_Watson(X_train, Y_train, X_test, h):\r\n",
        "    import scipy\r\n",
        "    \"\"\" Funcion que implementa metodo de ventana de parzen para\r\n",
        "        para clasificación\r\n",
        "    X_train: es la matriz con las muestras de entrenamiento\r\n",
        "    Y_train: es un vector con los valores de salida pra cada una de las muestras de entrenamiento\r\n",
        "    X_test: es la matriz con las muestras de validación\r\n",
        "    h (float): ancho de h de la ventana\r\n",
        "    retorna: - las estimaciones del modelo parzen para el conjunto X_test \r\n",
        "              esta matriz debe tener un shape de [row/muestras de X_test]\r\n",
        "             - las probabilidades de la vetana [row/muestras de X_test, numero de clases]  \r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    Yest = np.zeros(X_test.shape[0])\r\n",
        "    distancias = scipy.spatial.distance.cdist(X_test, X_train) / h\r\n",
        "    #parzen=ParzenWindow(X_test,X_train,h,Y_train)\r\n",
        "    for i in range(len(X_test)):\r\n",
        "        pesos = kernel_gaussiano(distancias[i])\r\n",
        "        x = np.sum(np.multiply(pesos, Y_train))\r\n",
        "        y = np.sum(pesos)\r\n",
        "        Yest[i] = x/y   \r\n",
        "    \r\n",
        "    #Debe retornar un vector que contenga las predicciones para cada una de las muestras en X_val, en el mismo orden.  \r\n",
        "    return Yest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WDVM3MtpF8u"
      },
      "source": [
        "def experimentarParzen (X, Y, hs):\r\n",
        "    \"\"\"Función que realiza los experimentos con knn usando\r\n",
        "       una estrategia de validacion entrenamiento y pruebas\r\n",
        "    X: matriz de numpy conjunto con muestras y caracteristicas\r\n",
        "    Y: vector de numpy con los valores de las etiquetas\r\n",
        "    ks: List[int/float] lista con los valores de k-vecinos a usar\r\n",
        "    retorna: dataframe con los resultados, debe contener las siguientes columnas:\r\n",
        "        - el ancho de ventana, \r\n",
        "        - el error medio de prueba\r\n",
        "        - la desviacion estandar del error\r\n",
        "        - número de promedio en el conjunto de prueba/validacion\r\n",
        "    \"\"\"\r\n",
        "    # se usa la función para implementar la estrategia de validación.\r\n",
        "    kfolds = KFold(n_splits=26)\r\n",
        "    resultados = pd.DataFrame()\r\n",
        "    idx = 0\r\n",
        "    # iteramos sobre los valores de hs\r\n",
        "    for h in hs:\r\n",
        "        # lista para almacenar los errores y numero de muestras\r\n",
        "        # de cada iteración\r\n",
        "        # de la validación\r\n",
        "        error_temp = []\r\n",
        "        numero_muestras = []\r\n",
        "        \r\n",
        "        for train, test in kfolds.split(X,Y ):\r\n",
        "\r\n",
        "            Xtrain = X[train,:]\r\n",
        "            Ytrain = Y[train]\r\n",
        "            Xtest = X[test,:]\r\n",
        "            Ytest = Y[test]\r\n",
        "            #normalizamos los datos\r\n",
        "            scaler = StandardScaler()\r\n",
        "            scaler.fit(Xtrain)\r\n",
        "            Xtrain = scaler.transform(Xtrain)\r\n",
        "            Xtest = scaler.transform(Xtest)\r\n",
        "            Yest = Nadaraya_Watson(Xtrain,Ytrain,Xtest,h)\r\n",
        "            errorTest = MAPE(Yest,Ytest)\r\n",
        "            error_temp.append(errorTest)\r\n",
        "            numero_muestras.append(len(Xtest))\r\n",
        "            \r\n",
        "    \r\n",
        "        resultados.loc[idx,'ancho de ventana'] = h \r\n",
        "        resultados.loc[idx,'error de prueba(media)'] = np.mean(error_temp)\r\n",
        "        resultados.loc[idx,'error de prueba(desviación estandar)'] =np.std(error_temp)\r\n",
        "        resultados.loc[idx,'muestras en conjunto de pruebas (media)'] = np.mean(numero_muestras)\r\n",
        "       \r\n",
        "        idx+=1\r\n",
        "    return (resultados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z4hlHz3pHrm"
      },
      "source": [
        "hs = [1,1.5 ,2.5, 5, 10]\r\n",
        "experimentos_parzen = experimentarParzen(X,Y, hs)\r\n",
        "experimentos_parzen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DobUNrBgBK_x"
      },
      "source": [
        "#pca\r\n",
        "def extract_features(tipo, n):\r\n",
        "    \"\"\"Feature Extraction Function\r\n",
        "    Recibe 2 parámetros: 1. el tipo de método de extracción (pca o lda como string), 2. el número componentes (para pca)\r\n",
        "    o el número de discriminantes (para lda)\r\n",
        "    \"\"\"\r\n",
        "    if tipo == 'pca':\r\n",
        "    \r\n",
        "        ext = PCA(n_components=n)\r\n",
        "    \r\n",
        "        return ext\r\n",
        "\r\n",
        "    elif tipo == 'lda':\r\n",
        "        \r\n",
        "        ext = LDA(n_components=n)\r\n",
        "        \r\n",
        "        return ext\r\n",
        "    \r\n",
        "    else:\r\n",
        "        print (\"Ingrese un método válido (pca o lda)\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gptMCMHhBAAH"
      },
      "source": [
        "std = scale(X)\r\n",
        "\r\n",
        "#Aquí se aplica la extracción de características por PCA\r\n",
        "ex = extract_features('lda',14)#Complete el código llamando el método extract_features. Tenga en cuenta lo que le pide el ejercicio 3.1\r\n",
        "#Fit de PCA\r\n",
        "ex = ex.fit(X,Y) #Complete el código con el fit correspondiente\r\n",
        "\r\n",
        "#Transforme las variables y genere el nuevo espacio de características de menor dimensión\r\n",
        "X_ex = ex.transform(X) #complete el código aquí para hacer la transformación"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_coIAEGApdl"
      },
      "source": [
        "#pca mquina de  vectores\r\n",
        "from sklearn import preprocessing\r\n",
        "from sklearn.svm import SVC\r\n",
        "def entrenamiento4(kernel, c, gamma):\r\n",
        "\r\n",
        "    Folds = 4\r\n",
        "    random.seed(19680801)\r\n",
        "    EficienciaTrain = np.zeros(Folds)\r\n",
        "    EficienciaVal = np.zeros(Folds)\r\n",
        "    Eficiencia=np.zeros(Folds)\r\n",
        "    Precision=np.zeros(Folds)\r\n",
        "    Sensibilidad=np.zeros(Folds)\r\n",
        "    Fscore=np.zeros(Folds)\r\n",
        "    Porcentaje=np.zeros(Folds)\r\n",
        "    skf = StratifiedKFold(n_splits=Folds)\r\n",
        "    j = 0\r\n",
        "    for train, test in skf.split(X_ex, Y):\r\n",
        "        \r\n",
        "        \r\n",
        "        Xtrain = X_ex[train,:]\r\n",
        "        Ytrain = Y[train]\r\n",
        "        Xtest = X_ex[test,:]\r\n",
        "        Ytest = Y[test]\r\n",
        "\r\n",
        "        #Normalizamos los datos\r\n",
        "        media = np.mean(Xtrain)\r\n",
        "        desvia = np.std(Xtrain)\r\n",
        "        #Xtrain = sc.stats.stats.zscore(Xtrain)\r\n",
        "        #Xtest = (Xtest - np.matlib.repmat(media, Xtest.shape[0], 1))/np.matlib.repmat(desvia, Xtest.shape[0], 1)\r\n",
        "\r\n",
        "        #Haga el llamado a la función para crear y entrenar el modelo usando los datos de entrenamiento\r\n",
        "\r\n",
        "        \r\n",
        "        if( kernel == 'linear'):\r\n",
        "            modelo = SVC(kernel= kernel, C=c, decision_function_shape='ovo')#Si es Lineal\r\n",
        "        else:\r\n",
        "            modelo = SVC(kernel= kernel, C=c, gamma=gamma, decision_function_shape='ovo') #Si es rbf\r\n",
        "      \r\n",
        "    \r\n",
        "    \r\n",
        "        modelo.fit(Xtrain,Ytrain)\r\n",
        "        \r\n",
        "        Ytrain_pred = modelo.predict(Xtrain)        #Validación con las muestras de entrenamiento\r\n",
        "        Yest = modelo.predict(Xtest)        #Validación con las muestras de test  \r\n",
        "        \r\n",
        "        Eficiencia[j] = accuracy_score(Ytest, Yest)\r\n",
        "        Precision[j] =np.mean(precision_score(Ytest, Yest, average=None))\r\n",
        "        Sensibilidad[j]=np.mean(recall_score(Ytest, Yest, average=None))\r\n",
        "        Fscore[j]=np.mean(f1_score(Ytest, Yest, average=None))\r\n",
        "    \r\n",
        "        #Evaluamos las predicciones del modelo con los datos de test\r\n",
        "        EficienciaVal[j] = np.mean(Yest.ravel() == Ytest.ravel())\r\n",
        "        Porcentaje[j] =  len(modelo.support_vectors_) / len(Xtrain)\r\n",
        "        j += 1\r\n",
        "\r\n",
        "    \r\n",
        "    eficiencia = np.mean(EficienciaVal)\r\n",
        "    intervaloConfianza = np.std(EficienciaVal)\r\n",
        "    porcentaje = np.mean(Porcentaje)\r\n",
        "    \r\n",
        "    return eficiencia, intervaloConfianza, np.mean(Eficiencia),np.mean(Precision), np.mean(Sensibilidad), np.mean(Fscore), porcentaje"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAmyb5LHAonY"
      },
      "source": [
        "df_types2 = pd.DataFrame({\r\n",
        "    'Kernel' : pd.Series(['linear','rbf']),\r\n",
        "    'C' : pd.Series([1,100]),\r\n",
        "    'gamma' : pd.Series([0,0.1])})\r\n",
        "df_types2[\"% de Vectores de Soporte\"] = \"\"\r\n",
        "df_types2[\"Eficiencia en validacion\"] = \"\"\r\n",
        "df_types2[\"Intervalo de confianza\"] = \"\"\r\n",
        "df_types2[\"Eficiencia\"] = \"\"\r\n",
        "df_types2[\"Precisiòn\"] = \"\"\r\n",
        "df_types2[\"Sensibilidad\"] = \"\"\r\n",
        "df_types2[\"Fscore\"] = \"\"\r\n",
        "\r\n",
        "df_types2.set_index(['Kernel','C','gamma'], inplace=True)\r\n",
        "#df_types.sort_index(inplace=True)\r\n",
        "\r\n",
        "arrEficienciaVal = []\r\n",
        "arrInConfianza= []\r\n",
        "arrEficiencia=[]\r\n",
        "arrPrecision=[]\r\n",
        "arrSensibilidad=[]\r\n",
        "arrFscore=[]\r\n",
        "arrPorcentaje=[]\r\n",
        "\r\n",
        "for h in df_types2.index:\r\n",
        "    eficienciaVal, intervaloConfianza,Eficiencia, Precision, Sensibilidad, Fscore, porcentaje = entrenamiento4(h[0],h[1],h[2])   \r\n",
        "    arrEficienciaVal.append(eficienciaVal)\r\n",
        "    arrInConfianza.append(intervaloConfianza)\r\n",
        "    arrEficiencia.append(Eficiencia)\r\n",
        "    arrPrecision.append(Precision)\r\n",
        "    arrSensibilidad.append(Sensibilidad)\r\n",
        "    arrFscore.append(Fscore)\r\n",
        "    arrPorcentaje.append(porcentaje)\r\n",
        "        \r\n",
        "df_types2[\"Eficiencia en validacion\"] = arrEficienciaVal\r\n",
        "df_types2[\"Intervalo de confianza\"] = arrInConfianza\r\n",
        "df_types2[\"Eficiencia\"] = arrEficiencia\r\n",
        "df_types2[\"Precisiòn\"] = arrPrecision\r\n",
        "df_types2[\"Sensibilidad\"] = arrSensibilidad\r\n",
        "df_types2[\"Fscore\"] = arrFscore\r\n",
        "df_types2[\"% de Vectores de Soporte\"] = arrPorcentaje"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXfSwxrECSNb"
      },
      "source": [
        "qgrid_widget = qgrid.show_grid(df_types2, show_toolbar=False)\r\n",
        "qgrid_widget\r\n",
        "qgrid_widget.get_changed_df()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}